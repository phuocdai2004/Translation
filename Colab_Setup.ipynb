{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbddf306",
   "metadata": {},
   "source": [
    "# üöÄ Machine Translation & Document Search on Google Colab\n",
    "\n",
    "Ch·∫°y h·ªá th·ªëng Machine Translation + Document Search tr√™n Google Colab\n",
    "\n",
    "**Y√™u c·∫ßu:** \n",
    "- Google Account\n",
    "- GPU Colab (khuy·∫øn ngh·ªã)\n",
    "- Kho·∫£ng 30 ph√∫t l·∫ßn ƒë·∫ßu (download models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66288b97",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ C√†i ƒê·∫∑t Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d208bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫≠p nh·∫≠t pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install -q \\\n",
    "    fastapi==0.104.1 \\\n",
    "    uvicorn==0.24.0 \\\n",
    "    python-multipart==0.0.6 \\\n",
    "    sqlalchemy==2.0.23 \\\n",
    "    sentence-transformers==2.2.2 \\\n",
    "    torch==2.0.1 \\\n",
    "    transformers==4.35.0 \\\n",
    "    annoy==1.17.3 \\\n",
    "    numpy==1.24.3 \\\n",
    "    pydantic==2.4.2 \\\n",
    "    httpx==0.25.0\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb4d00",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Project T·ª´ GitHub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d60363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone project n·∫øu mu·ªën d√πng code t·ª´ GitHub\n",
    "# !git clone https://github.com/phuocdai2004/haystack.git\n",
    "# %cd haystack/backend\n",
    "\n",
    "# Ho·∫∑c t·∫°o project structure c·ª•c b·ªô\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c\n",
    "os.makedirs('haystack_colab/app/models', exist_ok=True)\n",
    "os.makedirs('haystack_colab/app/routes', exist_ok=True)\n",
    "os.makedirs('haystack_colab/app/services', exist_ok=True)\n",
    "os.makedirs('haystack_colab/app/utils', exist_ok=True)\n",
    "os.makedirs('haystack_colab/data', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Project structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecba1f1",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ T·∫°o Database Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7061b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o database.py\n",
    "database_code = '''\n",
    "\"\"\"Database configuration for Colab\"\"\"\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from typing import Generator\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# SQLite database\n",
    "DATABASE_URL = \"sqlite:///./haystack.db\"\n",
    "\n",
    "engine = create_engine(\n",
    "    DATABASE_URL,\n",
    "    connect_args={\"check_same_thread\": False},\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(\n",
    "    bind=engine,\n",
    "    class_=Session,\n",
    "    expire_on_commit=False\n",
    ")\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Initialize database\"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS document (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    title VARCHAR(255),\n",
    "                    content TEXT NOT NULL,\n",
    "                    language VARCHAR(50) NOT NULL DEFAULT 'vi',\n",
    "                    doc_metadata JSON,\n",
    "                    embedding BLOB,\n",
    "                    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\"))\n",
    "            \n",
    "            conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_document_language ON document(language)\"))\n",
    "            conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_document_title ON document(title)\"))\n",
    "            conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_document_created_at ON document(created_at)\"))\n",
    "        \n",
    "        logger.info(\"Database initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database initialization error: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_session() -> Generator[Session, None, None]:\n",
    "    \"\"\"Get database session\"\"\"\n",
    "    with SessionLocal() as session:\n",
    "        yield session\n",
    "'''\n",
    "\n",
    "with open('haystack_colab/app/database.py', 'w') as f:\n",
    "    f.write(database_code)\n",
    "\n",
    "print(\"‚úÖ Database module created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ae80a",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ T·∫°o Embedding Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o embedding_service.py\n",
    "embedding_service_code = '''\n",
    "\"\"\"Embedding service for semantic search\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "embedding_model = None\n",
    "\n",
    "def init_embeddings():\n",
    "    \"\"\"Initialize embedding model\"\"\"\n",
    "    global embedding_model\n",
    "    if embedding_model is None:\n",
    "        try:\n",
    "            logger.info(\"Loading embedding model...\")\n",
    "            embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "            logger.info(f\"‚úì Model loaded: {embedding_model.get_sentence_embedding_dimension()} dims\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            embedding_model = None\n",
    "    return embedding_model\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Generate embedding for text\"\"\"\n",
    "    model = init_embeddings()\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Embedding model not available\")\n",
    "    \n",
    "    embedding = model.encode(text, convert_to_numpy=True)\n",
    "    return embedding\n",
    "\n",
    "def get_embeddings_batch(texts: list) -> list:\n",
    "    \"\"\"Generate embeddings for multiple texts\"\"\"\n",
    "    model = init_embeddings()\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Embedding model not available\")\n",
    "    \n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    return embeddings.tolist()\n",
    "'''\n",
    "\n",
    "os.makedirs('haystack_colab/app/services', exist_ok=True)\n",
    "with open('haystack_colab/app/services/embedding_service.py', 'w') as f:\n",
    "    f.write(embedding_service_code)\n",
    "\n",
    "print(\"‚úÖ Embedding service created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19559f62",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ T·∫°o Translation Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o translation_service.py\n",
    "translation_service_code = '''\n",
    "\"\"\"Translation service\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "translation_models = {}\n",
    "\n",
    "def load_translation_model(model_name: str):\n",
    "    \"\"\"Load translation model\"\"\"\n",
    "    if model_name not in translation_models:\n",
    "        try:\n",
    "            logger.info(f\"Loading {model_name}...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            translation_models[model_name] = (tokenizer, model)\n",
    "            logger.info(f\"‚úì Model loaded: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {model_name}: {e}\")\n",
    "            raise\n",
    "    return translation_models[model_name]\n",
    "\n",
    "def translate(text: str, source_lang: str, target_lang: str) -> str:\n",
    "    \"\"\"Translate text\"\"\"\n",
    "    if source_lang == \"en\" and target_lang == \"vi\":\n",
    "        model_name = \"Helsinki-NLP/opus-mt-en-vi\"\n",
    "    elif source_lang == \"vi\" and target_lang == \"en\":\n",
    "        model_name = \"Helsinki-NLP/opus-mt-vi-en\"\n",
    "    else:\n",
    "        raise ValueError(f\"Language pair {source_lang}-{target_lang} not supported\")\n",
    "    \n",
    "    tokenizer, model = load_translation_model(model_name)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return result\n",
    "'''\n",
    "\n",
    "with open('haystack_colab/app/services/translation_service.py', 'w') as f:\n",
    "    f.write(translation_service_code)\n",
    "\n",
    "print(\"‚úÖ Translation service created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff85f60",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ T·∫°o FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86814bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o main.py\n",
    "main_code = '''\n",
    "\"\"\"Main FastAPI application\"\"\"\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Th√™m ƒë∆∞·ªùng d·∫´n\n",
    "sys.path.insert(0, os.path.dirname(__file__))\n",
    "\n",
    "from app.database import init_db, SessionLocal, engine\n",
    "from app.services.translation_service import translate\n",
    "from app.services.embedding_service import get_embedding, get_embeddings_batch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(title=\"Translation & Document Search API\")\n",
    "\n",
    "# CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request/Response models\n",
    "class TranslationRequest(BaseModel):\n",
    "    text: str\n",
    "    source_lang: str\n",
    "    target_lang: str\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    source_lang: str\n",
    "    target_lang: str\n",
    "\n",
    "class EmbeddingRequest(BaseModel):\n",
    "    texts: list[str]\n",
    "\n",
    "class DocumentUploadRequest(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    language: str = \"vi\"\n",
    "\n",
    "# Initialize database\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    logger.info(\"Initializing database...\")\n",
    "    init_db()\n",
    "    logger.info(\"Application startup complete\")\n",
    "\n",
    "# Routes\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"ok\", \"message\": \"API is running on Colab!\"}\n",
    "\n",
    "@app.post(\"/api/translate\")\n",
    "async def translate_text(request: TranslationRequest):\n",
    "    try:\n",
    "        result = translate(request.text, request.source_lang, request.target_lang)\n",
    "        return TranslationResponse(\n",
    "            source=request.text,\n",
    "            target=result,\n",
    "            source_lang=request.source_lang,\n",
    "            target_lang=request.target_lang\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Translation error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/embed\")\n",
    "async def get_embeddings(request: EmbeddingRequest):\n",
    "    try:\n",
    "        embeddings = get_embeddings_batch(request.texts)\n",
    "        return {\n",
    "            \"texts\": request.texts,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"dimension\": 384\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Embedding error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/documents\")\n",
    "async def upload_document(request: DocumentUploadRequest):\n",
    "    try:\n",
    "        from sqlalchemy import text\n",
    "        \n",
    "        # T·∫°o embedding\n",
    "        embedding = get_embedding(request.content)\n",
    "        \n",
    "        # L∆∞u v√†o database\n",
    "        with SessionLocal() as session:\n",
    "            session.execute(text(\"\"\"\n",
    "                INSERT INTO document (title, content, language, embedding)\n",
    "                VALUES (:title, :content, :language, :embedding)\n",
    "            \"\"\"), {\n",
    "                \"title\": request.title,\n",
    "                \"content\": request.content,\n",
    "                \"language\": request.language,\n",
    "                \"embedding\": embedding.tobytes()\n",
    "            })\n",
    "            session.commit()\n",
    "        \n",
    "        return {\"status\": \"success\", \"message\": \"Document uploaded\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Upload error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    logger.info(\"Starting FastAPI server...\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "with open('haystack_colab/main.py', 'w') as f:\n",
    "    f.write(main_code)\n",
    "\n",
    "print(\"‚úÖ Main application created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfa7bf",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ T·∫°o __init__.py Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o __init__.py files\n",
    "import os\n",
    "\n",
    "for dir_path in [\n",
    "    'haystack_colab',\n",
    "    'haystack_colab/app',\n",
    "    'haystack_colab/app/models',\n",
    "    'haystack_colab/app/routes',\n",
    "    'haystack_colab/app/services',\n",
    "    'haystack_colab/app/utils'\n",
    "]:\n",
    "    init_file = os.path.join(dir_path, '__init__.py')\n",
    "    if not os.path.exists(init_file):\n",
    "        with open(init_file, 'w') as f:\n",
    "            f.write('')\n",
    "\n",
    "print(\"‚úÖ __init__.py files created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a6521",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Download Models (T√πy Ch·ªçn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b808f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3692e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian sau n√†y\n",
    "print(\"‚è≥ Downloading translation models (this may take a few minutes)...\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "try:\n",
    "    print(\"Downloading Helsinki-NLP/opus-mt-en-vi...\")\n",
    "    AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\")\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\")\n",
    "    print(\"‚úì EN‚ÜíVI model downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error downloading EN‚ÜíVI: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nDownloading Helsinki-NLP/opus-mt-vi-en...\")\n",
    "    AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-vi-en\")\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-vi-en\")\n",
    "    print(\"‚úì VI‚ÜíEN model downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error downloading VI‚ÜíEN: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nDownloading Sentence Transformers embeddings model...\")\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(\"‚úì Embeddings model downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error downloading embeddings: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d069050",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Test Translation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd47e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Testing Translation API...\\n\")\n",
    "\n",
    "# Test EN ‚Üí VI\n",
    "print(\"üìù Test 1: Translate English ‚Üí Vietnamese\")\n",
    "model_name_en_vi = \"Helsinki-NLP/opus-mt-en-vi\"\n",
    "tokenizer_en_vi = AutoTokenizer.from_pretrained(model_name_en_vi)\n",
    "model_en_vi = AutoModelForSeq2SeqLM.from_pretrained(model_name_en_vi)\n",
    "\n",
    "text_en = \"Hello, how are you today?\"\n",
    "inputs = tokenizer_en_vi(text_en, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model_en_vi.generate(**inputs, max_length=512)\n",
    "text_vi = tokenizer_en_vi.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input (EN): {text_en}\")\n",
    "print(f\"Output (VI): {text_vi}\")\n",
    "print()\n",
    "\n",
    "# Test VI ‚Üí EN\n",
    "print(\"üìù Test 2: Translate Vietnamese ‚Üí English\")\n",
    "model_name_vi_en = \"Helsinki-NLP/opus-mt-vi-en\"\n",
    "tokenizer_vi_en = AutoTokenizer.from_pretrained(model_name_vi_en)\n",
    "model_vi_en = AutoModelForSeq2SeqLM.from_pretrained(model_name_vi_en)\n",
    "\n",
    "text_vi_input = \"Xin ch√†o, b·∫°n kh·ªèe kh√¥ng?\"\n",
    "inputs = tokenizer_vi_en(text_vi_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model_vi_en.generate(**inputs, max_length=512)\n",
    "text_en_output = tokenizer_vi_en.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input (VI): {text_vi_input}\")\n",
    "print(f\"Output (EN): {text_en_output}\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ Translation API working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858f769",
   "metadata": {},
   "source": [
    "## üîü Test Embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Testing Embeddings API...\\n\")\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "print(f\"Model dimension: {embeddings[0].shape}\")\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text[:50]}...\")\n",
    "    print(f\"Embedding shape: {embeddings[i].shape}\")\n",
    "    print(f\"First 5 values: {embeddings[i][:5]}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Embeddings API working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dc299",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Ch·∫°y FastAPI Server Tr√™n Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y ngrok tunnel ƒë·ªÉ expose API\n",
    "!pip install -q pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# B·∫°n c·∫ßn ƒëƒÉng k√Ω t·∫°i https://dashboard.ngrok.com ƒë·ªÉ l·∫•y auth token\n",
    "# ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")\n",
    "\n",
    "print(\"‚è≥ Starting FastAPI server...\")\n",
    "print(\"Tip: C·∫ßn ngrok auth token ƒë·ªÉ ch·∫°y server tr·ª±c ti·∫øp\")\n",
    "print(\"Thay v√†o ƒë√≥, b·∫°n c√≥ th·ªÉ test API locally trong Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ef9ef",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Test API Locally (Recommended for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07379961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API locally m√† kh√¥ng c·∫ßn ch·∫°y server\n",
    "print(\"Testing API Functions Directly...\\n\")\n",
    "\n",
    "# Test 1: Translation\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def translate_func(text, source_lang, target_lang):\n",
    "    if source_lang == \"en\" and target_lang == \"vi\":\n",
    "        model_name = \"Helsinki-NLP/opus-mt-en-vi\"\n",
    "    elif source_lang == \"vi\" and target_lang == \"en\":\n",
    "        model_name = \"Helsinki-NLP/opus-mt-vi-en\"\n",
    "    else:\n",
    "        raise ValueError(\"Language pair not supported\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# Test translations\n",
    "test_cases = [\n",
    "    (\"Good morning\", \"en\", \"vi\"),\n",
    "    (\"Thank you very much\", \"en\", \"vi\"),\n",
    "    (\"T√¥i y√™u l·∫≠p tr√¨nh\", \"vi\", \"en\"),\n",
    "]\n",
    "\n",
    "for text, source, target in test_cases:\n",
    "    result = translate_func(text, source, target)\n",
    "    print(f\"[{source.upper()}‚Üí{target.upper()}] {text}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394f124",
   "metadata": {},
   "source": [
    "## üìå H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Tr√™n Colab\n",
    "\n",
    "### ‚úÖ ∆Øu ƒêi·ªÉm\n",
    "- ‚úì Free GPU t·ª´ Google\n",
    "- ‚úì Kh√¥ng c·∫ßn c√†i ƒë·∫∑t m√°y t√≠nh\n",
    "- ‚úì M√¥ h√¨nh t·ª± ƒë·ªông download\n",
    "- ‚úì D·ªÖ share v√† collaborative\n",
    "\n",
    "### ‚ö†Ô∏è H·∫°n Ch·∫ø\n",
    "- ‚ö† M·ªói session ch·ªâ ch·∫°y ~12 gi·ªù\n",
    "- ‚ö† C·∫ßn t√≠nh to√°n l·∫°i khi kh·ªüi ƒë·ªông l·∫°i\n",
    "- ‚ö† Kh√¥ng c√≥ persistent file system\n",
    "\n",
    "### üöÄ C√°ch Ch·∫°y Server Tr·ª±c Ti·∫øp (Advanced)\n",
    "\n",
    "```python\n",
    "# C·∫ßn install ngrok\n",
    "# 1. ƒêƒÉng k√Ω t·∫°i https://dashboard.ngrok.com\n",
    "# 2. L·∫•y auth token\n",
    "# 3. Ch·∫°y cell b√™n d∆∞·ªõi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0aecd",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Run Full Server (Advanced - C·∫ßn ngrok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°ch ch·∫°y full server tr√™n Colab\n",
    "\n",
    "full_server_code = '''\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# B∆∞·ªõc 1: Setup ngrok\n",
    "print(\"Setup ngrok...\")\n",
    "os.system(\"pip install -q pyngrok\")\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# ‚ö†Ô∏è Thay YOUR_AUTH_TOKEN b·∫±ng token c·ªßa b·∫°n t·ª´ https://dashboard.ngrok.com\n",
    "AUTH_TOKEN = \"YOUR_AUTH_TOKEN\"\n",
    "ngrok.set_auth_token(AUTH_TOKEN)\n",
    "\n",
    "# B∆∞·ªõc 2: Start FastAPI server\n",
    "print(\"Starting FastAPI server...\")\n",
    "subprocess.Popen([\"python\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"])\n",
    "\n",
    "# B∆∞·ªõc 3: Expose with ngrok\n",
    "print(\"Exposing with ngrok...\")\n",
    "public_url = ngrok.connect(8000, \"http\")\n",
    "print(f\"\\n‚úÖ Public URL: {public_url}\")\n",
    "print(f\"API is now accessible at: {public_url}/docs\")\n",
    "\n",
    "# B∆∞·ªõc 4: Keep server running\n",
    "import time\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "'''\n",
    "\n",
    "print(\"Code ƒë·ªÉ ch·∫°y full server tr√™n Colab:\")\n",
    "print(full_server_code)\n",
    "print(\"\\n‚ö†Ô∏è C·∫ßn edit AUTH_TOKEN tr∆∞·ªõc khi ch·∫°y!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed50d8",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "| Feature | Colab | Local Machine |\n",
    "|---------|-------|---------------|\n",
    "| GPU | ‚úÖ Free | T√πy c√≥/kh√¥ng |\n",
    "| C√†i ƒë·∫∑t | ‚úÖ T·ª± ƒë·ªông | ‚ö†Ô∏è Manual |\n",
    "| Persistent | ‚ö†Ô∏è 12h/session | ‚úÖ Vƒ©nh vi·ªÖn |\n",
    "| Server | ‚úÖ C√≥ (ngrok) | ‚úÖ Native |\n",
    "| Cost | ‚úÖ Free | ‚ö†Ô∏è ƒêi·ªán nƒÉng |\n",
    "\n",
    "## üéØ Recommend\n",
    "- **For Demo**: D√πng Colab (nhanh, free)\n",
    "- **For Production**: Ch·∫°y tr√™n local ho·∫∑c server ri√™ng"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
